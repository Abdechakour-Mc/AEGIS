{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "a03swTnwRqII",
        "outputId": "25b3bf3c-7c4a-441c-e7d9-c792b503e80b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.32.1-py3-none-any.whl (314 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes, accelerate\n",
            "Successfully installed accelerate-0.32.1 bitsandbytes-0.43.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "# Install the required libraries\n",
        "!pip install  bitsandbytes  accelerate gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcP4bo6hQcLI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0rym6EFQcNp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_jF8N_1QcQZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akkWQ8nzQcTF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKOxtYXMTSp7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fkd8uvAYaP80"
      },
      "source": [
        "# Setup The Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5C0JVWM8TmCQ",
        "outputId": "738b7843-0db9-4a44-c305-ab364600c9c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are using cuda. Good!\n"
          ]
        }
      ],
      "source": [
        "import io\n",
        "import tokenize\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import gradio as gr\n",
        "import torch.nn.functional as F\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "\n",
        "\n",
        "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score,accuracy_score\n",
        "from tqdm import tqdm\n",
        "from typing import List, Tuple, Dict\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "\n",
        "# _fn = \"final\"\n",
        "LENGTH = 512\n",
        "BATCH_SIZE = 16\n",
        "STEP_SIZE=1\n",
        "NUM_LABELS = 14\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "EMBEDDING_SIZE = 768\n",
        "HIDDEN_SIZE = 512\n",
        "CNN_FILTERS=128\n",
        "\n",
        "BERT_MODEL_ID = \"microsoft/codebert-base\"\n",
        "# UAE_MODEL_ID = \"WhereIsAI/UAE-Code-Large-V1\"\n",
        "\n",
        "# you must use cuda to run this code. if this returns false, you can not proceed.\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "\n",
        "if USE_CUDA:\n",
        "    print(\"You are using cuda. Good!\")\n",
        "else:\n",
        "    print('You are NOT using cuda! Some problems may occur.')\n",
        "\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pydLk8PcCJa"
      },
      "source": [
        "# Load Our Classifier (AEGIS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PBuciqCwTSsz"
      },
      "outputs": [],
      "source": [
        "# Define BERT layer\n",
        "class BertLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BertLayer, self).__init__()\n",
        "        self.bert_model = AutoModel.from_pretrained(BERT_MODEL_ID)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> Tuple[torch.Tensor, None]:\n",
        "        bert_encodings = self.bert_model(input_ids, attention_mask)\n",
        "        bert_last_hidden = bert_encodings['last_hidden_state']\n",
        "        return bert_last_hidden, None\n",
        "\n",
        "# Define CNN Encoder\n",
        "class CNNEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNEncoder, self).__init__()\n",
        "        self.embedding_size = EMBEDDING_SIZE\n",
        "        self.filter_number = CNN_FILTERS\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(in_channels=self.embedding_size, out_channels=self.filter_number, kernel_size=2, padding=\"same\")\n",
        "        self.conv2 = nn.Conv1d(in_channels=self.embedding_size, out_channels=self.filter_number, kernel_size=3, padding=\"same\")\n",
        "        self.conv3 = nn.Conv1d(in_channels=self.embedding_size, out_channels=self.filter_number, kernel_size=5, padding=\"same\")\n",
        "        self.conv4 = nn.Conv1d(in_channels=self.embedding_size, out_channels=self.filter_number, kernel_size=1, padding=\"same\")\n",
        "\n",
        "    def forward(self, bert_last_hidden: torch.Tensor) -> torch.Tensor:\n",
        "        trans_embedded = torch.transpose(bert_last_hidden, dim0=1, dim1=2)\n",
        "\n",
        "        convolve1 = self.activation(self.conv1(trans_embedded))\n",
        "        convolve2 = self.activation(self.conv2(trans_embedded))\n",
        "        convolve3 = self.activation(self.conv3(trans_embedded))\n",
        "        convolve4 = self.activation(self.conv4(trans_embedded))\n",
        "\n",
        "        convolve1 = torch.transpose(convolve1, dim0=1, dim1=2)\n",
        "        convolve2 = torch.transpose(convolve2, dim0=1, dim1=2)\n",
        "        convolve3 = torch.transpose(convolve3, dim0=1, dim1=2)\n",
        "        convolve4 = torch.transpose(convolve4, dim0=1, dim1=2)\n",
        "\n",
        "        output = torch.cat((convolve4, convolve1, convolve2, convolve3), dim=2)\n",
        "        return output\n",
        "\n",
        "# Define Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "# Define Transformer-based Middle layer\n",
        "class Middle(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Middle, self).__init__()\n",
        "        self.activation = nn.ReLU()\n",
        "        self.pos_encoder = PositionalEncoding(HIDDEN_SIZE, dropout=0.1)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(HIDDEN_SIZE, nhead=2, batch_first=True, dim_feedforward=2048, activation=\"relu\", dropout=0.1)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=2, enable_nested_tensor=False)\n",
        "\n",
        "    def forward(self, encodings: torch.Tensor, input_mask: torch.Tensor) -> torch.Tensor:\n",
        "        src = encodings * math.sqrt(HIDDEN_SIZE)\n",
        "        src = self.pos_encoder(src)\n",
        "        out = self.transformer_encoder(src, src_key_padding_mask=input_mask)\n",
        "        return out\n",
        "\n",
        "# Define Decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, num_labels: int = NUM_LABELS, dropout_p: float = 0.5):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.self_attention = nn.MultiheadAttention(embed_dim=HIDDEN_SIZE, num_heads=8, dropout=0.1, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.layer_norm = nn.LayerNorm(HIDDEN_SIZE)\n",
        "        self.score = nn.Linear(HIDDEN_SIZE, self.num_labels)\n",
        "\n",
        "    def forward(self, encodings: torch.Tensor, input_mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        encodings2 = encodings.clone()\n",
        "        context, attention_weights = self.self_attention(encodings2, encodings2, encodings2, key_padding_mask=input_mask, need_weights=True)\n",
        "        encodings2 = self.layer_norm(self.dropout(context)) + encodings2\n",
        "        sum_mask = (~input_mask).sum(1).unsqueeze(1)\n",
        "        sum_encoder = ((encodings2) * ((~input_mask).unsqueeze(2))).sum(1)\n",
        "        score = self.score(self.dropout(sum_encoder / sum_mask))\n",
        "        return score, attention_weights\n",
        "\n",
        "# Define the AEGIS model\n",
        "class AEGIS(nn.Module):\n",
        "    def __init__(self, num_labels: int = NUM_LABELS, dropout_p: float = 0.5):\n",
        "        super(AEGIS, self).__init__()\n",
        "        self.bert_layer = BertLayer()\n",
        "        self.cnn_encoder = CNNEncoder()\n",
        "        self.middle = Middle()\n",
        "        self.decoder = Decoder(num_labels=num_labels, dropout_p=dropout_p)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        bert_hidden, _ = self.bert_layer(input_ids, attention_mask)\n",
        "        encoder_output = self.cnn_encoder(bert_hidden)\n",
        "        output = self.middle(encoder_output, attention_mask == 0)\n",
        "        score, attention_weights = self.decoder(output, attention_mask == 0)\n",
        "        return score, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZZksHMmAT6gi",
        "outputId": "abcfb562-6a9b-47b0-89c4-b49fdbb53952"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Define our mapping from/to CWEs IDs\n",
        "index2target = {0: 'SAFE',\n",
        "  1: 'CWE-78',\n",
        "  2: 'CWE-605',\n",
        "  3: 'CWE-502',\n",
        "  4: 'CWE-377',\n",
        "  5: 'CWE-20',\n",
        "  6: 'CWE-259',\n",
        "  7: 'CWE-330',\n",
        "  8: 'CWE-703',\n",
        "  9: 'CWE-319',\n",
        "  10: 'CWE-400',\n",
        "  11: 'CWE-89',\n",
        "  12: 'CWE-327',\n",
        "  13: 'CWE-22'}\n",
        "target2index = {v:k for k,v in index2target.items()}\n",
        "\n",
        "\n",
        "# Load our pretrained weights\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "aegis = AEGIS(num_labels=len(target2index))\n",
        "aegis.load_state_dict(torch.load(\"/content/drive/MyDrive/AEGIS/aegis-scl_v4.pkl\").state_dict())\n",
        "if USE_CUDA:\n",
        "    aegis.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go9SIQCMcjpQ"
      },
      "source": [
        "# Load our inspecter LLM (Acts like a cybersecurity expert)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "47cef6f736a34425ab352e613775d122",
            "a95670540c604e60a9d3ecb13caa6ea7",
            "ba503f7d4176449cadf1ff5742a1a083",
            "2ab3774bd41147f7a218a720bdc3b5be",
            "661e1b5cb01c45fa9430937e98bf5bc4",
            "ba18f85235d6454ab0c759b3a38b8096",
            "24ec8267572c4c26aa0ce0c4d0684e0a",
            "008ec75042f24dd482274c50fab5427f",
            "8ab462847c2c4a068cfc5da71b3a838a",
            "a8510616eee84063ad1c1daace769a92",
            "0740996a33be469294b81e0b3c91b44d"
          ]
        },
        "id": "rLg1KFy_ZS5H",
        "outputId": "4cf0eb24-0a8c-4604-fd52-ed7a5e2417c4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "47cef6f736a34425ab352e613775d122",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Define the model and tokenizer\n",
        "model_name = \"Qwen/CodeQwen1.5-7B-Chat\"\n",
        "\n",
        "# Load the tokenizer\n",
        "llm_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model_4bit = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "text_generation_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_4bit,\n",
        "    tokenizer=llm_tokenizer,\n",
        "    use_cache=True,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "def your_query(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate a response for the given query using the text generation pipeline.\n",
        "\n",
        "    Args:\n",
        "        query (str): The input query for the model.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated response.\n",
        "    \"\"\"\n",
        "    sequences = text_generation_pipeline(\n",
        "        query,\n",
        "        max_length=512,\n",
        "        eos_token_id=llm_tokenizer.eos_token_id,\n",
        "    )\n",
        "    output = sequences[0]['generated_text']\n",
        "    return output\n",
        "\n",
        "\n",
        "def format_messages(messages: List[Dict[str, str]]) -> str:\n",
        "    \"\"\"\n",
        "    Format messages for the Llama-2 chat models.\n",
        "\n",
        "    Args:\n",
        "        messages (List[Dict[str, str]]): List of messages with 'role' and 'content'.\n",
        "\n",
        "    Returns:\n",
        "        str: The formatted prompt.\n",
        "    \"\"\"\n",
        "    prompt: List[str] = []\n",
        "\n",
        "    if messages[0][\"role\"] == \"system\":\n",
        "        content = \"\".join([\"<S>\\n\", messages[0][\"content\"], \"\\n</S>\\n\\n\", messages[1][\"content\"]])\n",
        "        messages = [{\"role\": messages[1][\"role\"], \"content\": content}] + messages[2:]\n",
        "\n",
        "    for user, answer in zip(messages[::2], messages[1::2]):\n",
        "        prompt.extend([\"<s>\", \"[INST] \", (user[\"content\"]).strip(), \" [/INST] \", (answer[\"content\"]).strip(), \"</s>\"])\n",
        "\n",
        "    prompt.extend([\"<s>\", \"<|im_start|> \", (messages[-1][\"content\"]).strip(), \" <|im_end|>\"])\n",
        "\n",
        "    return \"\".join(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joPPah3fev-2"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dWe0ROvjQcV0"
      },
      "outputs": [],
      "source": [
        "# Initialize tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_ID)\n",
        "\n",
        "def remove_comments_and_docstrings(source: str) -> str:\n",
        "    \"\"\"\n",
        "    Remove comments and docstrings from the provided Python source code.\n",
        "\n",
        "    Args:\n",
        "        source (str): The source code to process.\n",
        "\n",
        "    Returns:\n",
        "        str: The source code with comments and docstrings removed.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        io_obj = io.StringIO(source)\n",
        "        out = \"\"\n",
        "        prev_toktype = tokenize.INDENT\n",
        "        last_lineno = -1\n",
        "        last_col = 0\n",
        "        for tok in tokenize.generate_tokens(io_obj.readline):\n",
        "            token_type = tok[0]\n",
        "            token_string = tok[1]\n",
        "            start_line, start_col = tok[2]\n",
        "            end_line, end_col = tok[3]\n",
        "            if start_line > last_lineno:\n",
        "                last_col = 0\n",
        "            if start_col > last_col:\n",
        "                out += (\" \" * (start_col - last_col))\n",
        "            if token_type == tokenize.COMMENT:\n",
        "                pass\n",
        "            elif token_type == tokenize.STRING:\n",
        "                if prev_toktype != tokenize.INDENT:\n",
        "                    if prev_toktype != tokenize.NEWLINE:\n",
        "                        if start_col > 0:\n",
        "                            out += token_string\n",
        "            else:\n",
        "                out += token_string\n",
        "            prev_toktype = token_type\n",
        "            last_col = end_col\n",
        "            last_lineno = end_line\n",
        "        out = '\\n'.join(line for line in out.splitlines() if line.strip())\n",
        "        return out\n",
        "\n",
        "    except Exception as e:\n",
        "        print('Error:', e)\n",
        "        return \"\"\n",
        "\n",
        "def process_input(code: str, tokenizer) -> Tuple[str, torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Process the input code by removing comments and docstrings, and tokenize it for the model.\n",
        "\n",
        "    Args:\n",
        "        code (str): The source code to process.\n",
        "        tokenizer: The tokenizer to use for processing the code.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[str, torch.Tensor, torch.Tensor]: The processed code, input IDs, and attention mask.\n",
        "    \"\"\"\n",
        "    code = remove_comments_and_docstrings(code)\n",
        "\n",
        "    tok_in = tokenizer.encode_plus(code, add_special_tokens=True, max_length=LENGTH,\n",
        "                                   return_attention_mask=True, padding='max_length',\n",
        "                                   truncation=True, return_tensors='pt')\n",
        "\n",
        "    input_ids, attention_mask = tok_in.input_ids.cuda(), tok_in.attention_mask.cuda()\n",
        "\n",
        "    return code, input_ids, attention_mask\n",
        "\n",
        "\n",
        "\n",
        "# Define the function to get line scores based on attention weights\n",
        "def get_line_scores(code: str, attention_mask: List[int], attention_weights: torch.Tensor) -> List[float]:\n",
        "    \"\"\"\n",
        "    Calculate the attention score for each line of code.\n",
        "\n",
        "    Args:\n",
        "    - code (str): The input code as a string.\n",
        "    - attention_mask (List[int]): The attention mask as a list of integers.\n",
        "    - attention_weights (torch.Tensor): The attention weights tensor.\n",
        "\n",
        "    Returns:\n",
        "    - List[float]: A list of attention scores for each line of code.\n",
        "    \"\"\"\n",
        "\n",
        "    # Remove padding from attention weights using the mask\n",
        "    masked_attention_weights = attention_weights[:, attention_mask.astype(bool)]\n",
        "    masked_attention_weights = masked_attention_weights[attention_mask.astype(bool), :]\n",
        "\n",
        "    # Sum the attention weights across the tokens\n",
        "    token_attention = masked_attention_weights.sum(dim=0)\n",
        "\n",
        "    # Calculate line attention scores\n",
        "    line_scores = []\n",
        "    start = 0\n",
        "    end = 0\n",
        "\n",
        "    lines = code.split(\"\\n\")\n",
        "    for idx, line in enumerate(lines):\n",
        "        if idx != len(lines) - 1:\n",
        "            token_ids = tokenizer.encode(line + \"\\n\")\n",
        "        else:\n",
        "            token_ids = tokenizer.encode(line)\n",
        "\n",
        "        token_encodings = tokenizer.convert_ids_to_tokens(token_ids)[1:-1]\n",
        "        line_length = len(token_encodings)\n",
        "\n",
        "        if idx == 0:\n",
        "            start += 1\n",
        "            end = line_length + 1\n",
        "        else:\n",
        "            start = end\n",
        "            end += line_length\n",
        "\n",
        "        line_score = float(sum(token_attention[start:end]))\n",
        "        line_scores.append(line_score)\n",
        "    return line_scores\n",
        "\n",
        "# Define the function to normalize scores\n",
        "def normalize_scores(danger_scores: List[float]) -> List[float]:\n",
        "    \"\"\"\n",
        "    Normalize a list of danger scores to a range of [0, 1].\n",
        "\n",
        "    Args:\n",
        "        danger_scores (List[float]): List of danger scores to normalize.\n",
        "\n",
        "    Returns:\n",
        "        List[float]: Normalized danger scores.\n",
        "    \"\"\"\n",
        "    min_score = min(danger_scores)\n",
        "    max_score = max(danger_scores)\n",
        "    if min_score == max_score:\n",
        "        return [0.0 for _ in danger_scores]\n",
        "    return [(score - min_score) / (max_score - min_score) for score in danger_scores]\n",
        "\n",
        "# Define the function to color text based on danger scores\n",
        "def color_text_by_danger(text_lines: List[str], danger_scores: List[float]) -> str:\n",
        "    \"\"\"\n",
        "    Colorize text lines based on normalized danger scores.\n",
        "\n",
        "    Args:\n",
        "        text_lines (List[str]): List of text lines to color.\n",
        "        danger_scores (List[float]): List of normalized danger scores corresponding to each text line.\n",
        "\n",
        "    Returns:\n",
        "        str: HTML content with colored text lines.\n",
        "    \"\"\"\n",
        "    normalized_scores = normalize_scores(danger_scores)\n",
        "\n",
        "    def get_color(score: float) -> str:\n",
        "        \"\"\"\n",
        "        Get an RGB color string based on the given score.\n",
        "\n",
        "        Args:\n",
        "            score (float): Normalized danger score.\n",
        "\n",
        "        Returns:\n",
        "            str: RGB color string in the format 'rgb(red,green,blue)'.\n",
        "        \"\"\"\n",
        "        red = int(255 * score)\n",
        "        green = int(255 * (1 - score))\n",
        "        return f'rgb({red},{green},0)'\n",
        "\n",
        "    html_content = \"\"\n",
        "    for line, score in zip(text_lines, normalized_scores):\n",
        "        color = get_color(score)\n",
        "        html_content += f'<pre style=\"color:{color}\">{line}</pre>'\n",
        "    return html_content\n",
        "\n",
        "# Define the text classification function\n",
        "def classify_text(code: str) -> Tuple[Dict[str, float], str]:\n",
        "    \"\"\"\n",
        "    Perform text classification on the given code snippet.\n",
        "\n",
        "    Args:\n",
        "        code (str): Input code snippet to classify.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict[str, float], str]: Tuple containing a dictionary of class probabilities and colored HTML code.\n",
        "    \"\"\"\n",
        "    code, input_ids, attention_mask = process_input(code, tokenizer)\n",
        "    score, attention_weights = aegis(input_ids, attention_mask)\n",
        "\n",
        "    probs = F.softmax(score, dim=1).squeeze().tolist()\n",
        "    class_proba_dict = {index2target[i]: prob for i, prob in enumerate(probs)}\n",
        "    most_probable_class = max(class_proba_dict, key=class_proba_dict.get)\n",
        "\n",
        "    line_scores = get_line_scores(code, attention_mask.cpu().squeeze().numpy(), attention_weights.squeeze().detach().cpu())\n",
        "    if most_probable_class == \"SAFE\":\n",
        "        line_scores = [1 for _ in line_scores]\n",
        "\n",
        "    colored_code = color_text_by_danger(code.split('\\n'), line_scores)\n",
        "    return class_proba_dict, colored_code\n",
        "\n",
        "# Define the function to fix text based on probabilities\n",
        "def fix_text(code: str, normalized_probabilities: Dict[str, float]) -> str:\n",
        "    \"\"\"\n",
        "    Fix the input code snippet based on normalized vulnerability probabilities.\n",
        "\n",
        "    Args:\n",
        "        code (str): Input code snippet to fix.\n",
        "        normalized_probabilities (Dict[str, float]): Dictionary of normalized vulnerability probabilities.\n",
        "\n",
        "    Returns:\n",
        "        str: Fixed code snippet or explanation.\n",
        "    \"\"\"\n",
        "    most_probable_class = max(normalized_probabilities, key=normalized_probabilities.get)\n",
        "    if most_probable_class == \"SAFE\":\n",
        "        prompt = f\"Why is this code safe from any vulnerability?\\nCode:{code}\"\n",
        "    else:\n",
        "        prompt = f\"If the code below is vulnerable to {most_probable_class}, explain why and suggest a fix.\\nCode:{code}\"\n",
        "\n",
        "    dialog = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "    formatted_prompt = format_messages(dialog)\n",
        "    generated_text = your_query(formatted_prompt)\n",
        "    return generated_text[len(formatted_prompt):].strip()\n",
        "\n",
        "# Define the function to update chat history\n",
        "def update_chatbox(fixed_text: str, chat_history: List[Tuple[str, str]]) -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Update the chat history with the fixed text.\n",
        "\n",
        "    Args:\n",
        "        fixed_text (str): Fixed text or response to add to chat history.\n",
        "        chat_history (List[Tuple[str, str]]): List of tuples representing chat history entries.\n",
        "\n",
        "    Returns:\n",
        "        List[Tuple[str, str]]: Updated chat history with the new entry.\n",
        "    \"\"\"\n",
        "    now = datetime.now()\n",
        "    chat_history.append((now.strftime(\"%B %d, %Y %H:%M:%S\"), fixed_text))\n",
        "    return chat_history\n",
        "\n",
        "# Define the function to regenerate fixed text and update chat history\n",
        "def regenerate_fixed_text(code: str, normalized_probabilities: Dict[str, float], chat_history: List[Tuple[str, str]] = []) -> Tuple[str, List[Tuple[str, str]]]:\n",
        "    \"\"\"\n",
        "    Regenerate the fixed text based on vulnerability probabilities and update chat history.\n",
        "\n",
        "    Args:\n",
        "        code (str): Input code snippet to fix.\n",
        "        normalized_probabilities (Dict[str, float]): Dictionary of normalized vulnerability probabilities.\n",
        "        chat_history (List[Tuple[str, str]], optional): List of tuples representing chat history entries. Defaults to [].\n",
        "\n",
        "    Returns:\n",
        "        Tuple[str, List[Tuple[str, str]]]: Tuple containing the fixed code snippet or explanation and updated chat history.\n",
        "    \"\"\"\n",
        "    fixed_text = fix_text(code, normalized_probabilities)\n",
        "    chat_history = update_chatbox(fixed_text, chat_history)\n",
        "    return fixed_text, chat_history\n",
        "\n",
        "# Define the function to get HTML content\n",
        "def get_html(html: str) -> str:\n",
        "    return html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6n1P4Xnc2eu"
      },
      "source": [
        "# Setup  Our Framework Interface (with the help of Gradio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "Rwxdn59DergQ",
        "outputId": "74f00db2-faf8-49d8-99d7-9c0628e4c142"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://019081c82251b8e66e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://019081c82251b8e66e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://019081c82251b8e66e.gradio.live\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create the Gradio interface\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    with gr.Row():\n",
        "        code_input = gr.TextArea(placeholder=\"Your Python Code?\", label=\"Input\")\n",
        "        class_output = gr.Label()\n",
        "        hidden_output = gr.Text(visible=False)\n",
        "    analyze_button = gr.Button(\"Analyze\")\n",
        "    analyze_button.click(fn=classify_text, inputs=code_input, outputs=[class_output, hidden_output])\n",
        "\n",
        "    gr.Examples(\n",
        "        examples=[[\"print('Hello, World!')\"]],\n",
        "        inputs=code_input\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        line_level_output = gr.Markdown(label=\"Line-level Analysis\")\n",
        "\n",
        "    line_level_button = gr.Button(\"Line-level Detection\")\n",
        "    line_level_button.click(fn=get_html, inputs=hidden_output, outputs=line_level_output)\n",
        "\n",
        "    with gr.Row():\n",
        "        insight_output = gr.Markdown(label=\"Suggestions\")\n",
        "\n",
        "    insight_button = gr.Button(\"Insights\")\n",
        "    chat_history_box = gr.Chatbot(label=\"Chat History\")\n",
        "    insight_button.click(fn=regenerate_fixed_text, inputs=[code_input, class_output], outputs=[insight_output, chat_history_box])\n",
        "\n",
        "demo.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6_zHKKiROZ8l",
        "outputId": "a369a94c-c45d-4e7f-ca40-beffcfd0cc9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-4.36.1-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==1.0.1 (from gradio)\n",
            "  Downloading gradio_client-1.0.1-py3-none-any.whl (318 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.1/318.1 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx>=0.24.1 (from gradio)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.23.3)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.7/142.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.7.3)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.4.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.30.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.0.1->gradio) (2023.6.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==1.0.1->gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.14.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.18.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi->gradio)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi-cli>=0.0.2 (from fastapi->gradio)\n",
            "  Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
            "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi->gradio)\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi->gradio)\n",
            "  Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n",
            "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->gradio)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.1)\n",
            "Collecting httptools>=0.5.0 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=d9618b1fe92c9ae767e0b75542666f59865eea2a0776f47fe0307eaf52587ea5\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, uvloop, ujson, tomlkit, semantic-version, ruff, python-multipart, python-dotenv, orjson, httptools, h11, dnspython, aiofiles, watchfiles, uvicorn, starlette, httpcore, email_validator, httpx, gradio-client, fastapi-cli, fastapi, gradio\n",
            "Successfully installed aiofiles-23.2.1 dnspython-2.6.1 email_validator-2.1.1 fastapi-0.111.0 fastapi-cli-0.0.4 ffmpy-0.3.2 gradio-4.36.1 gradio-client-1.0.1 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 orjson-3.10.4 pydub-0.25.1 python-dotenv-1.0.1 python-multipart-0.0.9 ruff-0.4.8 semantic-version-2.10.0 starlette-0.37.2 tomlkit-0.12.0 ujson-5.10.0 uvicorn-0.30.1 uvloop-0.19.0 watchfiles-0.22.0 websockets-11.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntUpD-qQakbl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "collapsed": true,
        "id": "b1Ck4oxKW2hY",
        "outputId": "7c669ccf-112d-42c3-ca6e-50954c8643d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://7396c9ec6ae3994f6c.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://7396c9ec6ae3994f6c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# yes\n",
        "import gradio as gr\n",
        "import random\n",
        "\n",
        "# Define the text classification model\n",
        "def classify_text(text):\n",
        "    probabilities = {\n",
        "        \"Positive\": random.random(),\n",
        "        \"Negative\": random.random(),\n",
        "        \"Neutral\": random.random(),\n",
        "    }\n",
        "    total = sum(probabilities.values())\n",
        "    normalized_probabilities = {k: v / total for k, v in probabilities.items()}\n",
        "    return normalized_probabilities\n",
        "\n",
        "# Define the text fixing model\n",
        "def fix_text(text, most_probable_class):\n",
        "    return f\"before: \\n```python{text}```\\nafter: This is the fixed text for {most_probable_class} ...\"\n",
        "\n",
        "# Define a function to handle the complete workflow\n",
        "def classify_and_fix(text):\n",
        "    # Classify the input text\n",
        "    classification_probs = classify_text(text)\n",
        "    # Find the most probable classification\n",
        "    most_probable_class = max(classification_probs, key=classification_probs.get)\n",
        "    # Fix the text based on the most probable classification\n",
        "    fixed_text = fix_text(text, most_probable_class)\n",
        "    # Return all relevant information\n",
        "    return classification_probs, most_probable_class, fixed_text\n",
        "\n",
        "\n",
        "\n",
        "# Create the Gradio interface\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "# with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        input_text = gr.Textbox(label=\"Input Text\")\n",
        "    with gr.Row():\n",
        "        classify_button = gr.Button(\"Classify Text\")\n",
        "    with gr.Row():\n",
        "        ll = gr.Label(label=\"Classification Probabilities\")\n",
        "    with gr.Row():\n",
        "        fixed_text_output = gr.Markdown(label=\"Fixed Text\")\n",
        "    with gr.Row():\n",
        "        regenerate_button = gr.Button(\"Regenerate Text\")\n",
        "\n",
        "    chatbox = gr.Chatbot(label=\"Chat History\")\n",
        "\n",
        "    def update_chatbox(fixed_text, chat_history):\n",
        "        # Update the chat history with the new fixed text only\n",
        "        chat_history.append((\"Fixed Text\", fixed_text))\n",
        "        return chat_history\n",
        "\n",
        "    # Initial classification and text fixing\n",
        "    def handle_classification(text, chat_history=[]):\n",
        "        classification_probs, most_probable_class, fixed_text = classify_and_fix(text)\n",
        "        chat_history = update_chatbox(fixed_text, chat_history)\n",
        "        return classification_probs, most_probable_class, fixed_text, chat_history\n",
        "\n",
        "    # Regenerate the fixed text based on the most probable class\n",
        "    def regenerate_fixed_text(text, most_probable_class, chat_history=[]):\n",
        "        fixed_text = fix_text(text, most_probable_class)\n",
        "        chat_history = update_chatbox(fixed_text, chat_history)\n",
        "        return fixed_text, chat_history\n",
        "\n",
        "    # Store the most probable class in a state variable to be reused by the regenerate button\n",
        "    most_probable_class_state = gr.State()\n",
        "\n",
        "    classify_button.click(fn=handle_classification, inputs=[input_text], outputs=[ll, most_probable_class_state, fixed_text_output, chatbox])\n",
        "    regenerate_button.click(fn=regenerate_fixed_text, inputs=[input_text, most_probable_class_state], outputs=[fixed_text_output, chatbox])\n",
        "\n",
        "# Launch the Gradio app\n",
        "demo.launch()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "008ec75042f24dd482274c50fab5427f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0740996a33be469294b81e0b3c91b44d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15b8c0ad8b1a454aa469818c0ad07d39": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfc5b53efadf4a01a95427abef806e22",
            "placeholder": "​",
            "style": "IPY_MODEL_1e415645b5d34b6887ac366610caa794",
            "value": "generation_config.json: 100%"
          }
        },
        "15e623c3ac8841be95570e3c752760c1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "178851da104a424aafe4a816fdf3742b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_461f9bdabbb649d691530abb758b461d",
            "placeholder": "​",
            "style": "IPY_MODEL_fc67bcbbefaa4dfd852888ba60c70d13",
            "value": " 4/4 [01:12&lt;00:00, 17.04s/it]"
          }
        },
        "1e415645b5d34b6887ac366610caa794": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e56a99cf38d4fb7880fdb104dcfa02a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24ec8267572c4c26aa0ce0c4d0684e0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26614c47125e4fd0b45a92b302c9b173": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ab3774bd41147f7a218a720bdc3b5be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8510616eee84063ad1c1daace769a92",
            "placeholder": "​",
            "style": "IPY_MODEL_0740996a33be469294b81e0b3c91b44d",
            "value": " 4/4 [01:24&lt;00:00, 20.17s/it]"
          }
        },
        "2e4233d5fc354cfc9fa8fa288d759bc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f7b8878f2e5046a5a105da976f139d0f",
              "IPY_MODEL_9c4abf5612e9497282f29a15763daf23",
              "IPY_MODEL_178851da104a424aafe4a816fdf3742b"
            ],
            "layout": "IPY_MODEL_15e623c3ac8841be95570e3c752760c1"
          }
        },
        "4233bc7aa55245758f4b41c8b6a01958": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_15b8c0ad8b1a454aa469818c0ad07d39",
              "IPY_MODEL_93546286d17644e389ad20ba45f7215d",
              "IPY_MODEL_f8c2ddc243f244fcb1ed4ef3f6505687"
            ],
            "layout": "IPY_MODEL_26614c47125e4fd0b45a92b302c9b173"
          }
        },
        "461f9bdabbb649d691530abb758b461d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47cef6f736a34425ab352e613775d122": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a95670540c604e60a9d3ecb13caa6ea7",
              "IPY_MODEL_ba503f7d4176449cadf1ff5742a1a083",
              "IPY_MODEL_2ab3774bd41147f7a218a720bdc3b5be"
            ],
            "layout": "IPY_MODEL_661e1b5cb01c45fa9430937e98bf5bc4"
          }
        },
        "56696614ae41408f95c353f72df6f5fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "661e1b5cb01c45fa9430937e98bf5bc4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b4e887470684ca4868ab10049dd5cdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d4707bc0eef43669a55715976a9c84d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ab462847c2c4a068cfc5da71b3a838a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9277d69ae9b54cb3a115bfdbd2f1e192": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93546286d17644e389ad20ba45f7215d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d79779566f464f6ab736d70a4efff287",
            "max": 212,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_afa5842b613b457e9227f91fefe32d9f",
            "value": 212
          }
        },
        "9c4abf5612e9497282f29a15763daf23": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d4707bc0eef43669a55715976a9c84d",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d3cf2ecd7ace4c44a5727c9d7a6be4f2",
            "value": 4
          }
        },
        "a8510616eee84063ad1c1daace769a92": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a95670540c604e60a9d3ecb13caa6ea7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba18f85235d6454ab0c759b3a38b8096",
            "placeholder": "​",
            "style": "IPY_MODEL_24ec8267572c4c26aa0ce0c4d0684e0a",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "afa5842b613b457e9227f91fefe32d9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ba18f85235d6454ab0c759b3a38b8096": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba503f7d4176449cadf1ff5742a1a083": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_008ec75042f24dd482274c50fab5427f",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8ab462847c2c4a068cfc5da71b3a838a",
            "value": 4
          }
        },
        "cfc5b53efadf4a01a95427abef806e22": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3cf2ecd7ace4c44a5727c9d7a6be4f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d79779566f464f6ab736d70a4efff287": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7b8878f2e5046a5a105da976f139d0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e56a99cf38d4fb7880fdb104dcfa02a",
            "placeholder": "​",
            "style": "IPY_MODEL_9277d69ae9b54cb3a115bfdbd2f1e192",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "f8c2ddc243f244fcb1ed4ef3f6505687": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56696614ae41408f95c353f72df6f5fe",
            "placeholder": "​",
            "style": "IPY_MODEL_7b4e887470684ca4868ab10049dd5cdf",
            "value": " 212/212 [00:00&lt;00:00, 12.5kB/s]"
          }
        },
        "fc67bcbbefaa4dfd852888ba60c70d13": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
